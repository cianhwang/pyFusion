{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test model behavior in synthetic/real images.\n",
    "## found model output one certain value after training.\n",
    "## test general mse value\n",
    "\n",
    "import os\n",
    "from model import *\n",
    "from data_loader import *\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import toy_utils\n",
    "import torch.nn.functional as F\n",
    "from helper_funcs import *\n",
    "import pytorch_ssim\n",
    "\n",
    "def load_checkpoint(ckpt_path, model):\n",
    "    \n",
    "    ckpt_dir = 'ckpt/'+ckpt_path\n",
    "\n",
    "    print(\"[*] Loading model from {}\".format(ckpt_dir))\n",
    "\n",
    "    filename = 'rfc_model_best.pth.tar'\n",
    "    ckpt_path = os.path.join(ckpt_dir, filename)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "\n",
    "    # load variables from checkpoint\n",
    "    start_epoch = ckpt['epoch']\n",
    "    best_loss = ckpt['best_valid_mse']\n",
    "    print(\"current epoch: {} --- best loss: {}\".format(start_epoch, best_loss))\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    #optimizer.load_state_dict(ckpt['optim_state'])   \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = focusLocNet(0.17, 1, 256, 2).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading model from ckpt/0507_16_13\n",
      "current epoch: 288 --- best loss: 0.668776384197207\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('0507_16_13', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awnet import pwc_5x5_sigmoid_bilinear   # cm:import AWnet model\n",
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "AWnet = pwc_5x5_sigmoid_bilinear.pwc_residual().cuda()\n",
    "AWnet.load_state_dict(torch.load('awnet/fs_34_all_0.03036882.pkl'))\n",
    "AWnet = AWnet.eval()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fuseTwoImages(I, J_hat):\n",
    "    with torch.no_grad():\n",
    "        fusedTensor,warp,mask = AWnet(J_hat,I)\n",
    "    return fusedTensor, warp, mask\n",
    "\n",
    "def patchize(img):\n",
    "    imgs = []\n",
    "    H, W, C = img.shape\n",
    "    ph = H//2\n",
    "    pw = W//2\n",
    "    img_empty = np.zeros((H+200, W+200, C))\n",
    "    img_empty[100:-100, 100:-100] = img\n",
    "    img = img_empty\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            imgs.append(img[100+ph*i-50:100+ph*i+ph+50, 100+pw*j-32:100+pw*j+pw+32])\n",
    "    imgs = np.stack(imgs)\n",
    "    return imgs\n",
    "\n",
    "def depatchize(imgs, pd_h = 50, pd_w = 32):\n",
    "    ph = (imgs[0].shape[0]-2*pd_h)\n",
    "    pw = (imgs[0].shape[1]-2*pd_w)\n",
    "    img = np.zeros((ph*2, pw*2, 3))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            img[i*ph:i*ph+ph, j*pw:j*pw+pw] = imgs[i*2+j, pd_h:-pd_h, pd_w:-pd_w]\n",
    "            \n",
    "    return img\n",
    "    \n",
    "def image_fuse(a_batch, b_batch):\n",
    "    batch_size = a_batch.size(0)\n",
    "    c_batch = []\n",
    "    for k in range(batch_size):\n",
    "        a = a_batch[k]\n",
    "        b = b_batch[k]\n",
    "        a = a.cpu().detach().numpy().transpose(1, 2, 0) /2. + 0.5\n",
    "        b = b.cpu().detach().numpy().transpose(1, 2, 0) /2. + 0.5\n",
    "        \n",
    "        aa = patchize(a)\n",
    "        bb = patchize(b)\n",
    "        aa = torch.Tensor(aa.transpose(0, 3, 1, 2)).cuda()\n",
    "        bb = torch.Tensor(bb.transpose(0, 3, 1, 2)).cuda()\n",
    "\n",
    "        ccs = []\n",
    "        #wws = []\n",
    "        for i in range(4):\n",
    "            cc, ww, mask = fuseTwoImages(aa[i:i+1], bb[i:i+1])\n",
    "            ccs.append(cc[0])\n",
    "            #wws.append(ww[0])\n",
    "        cc = torch.stack(ccs)\n",
    "        #ww = torch.stack(wws)\n",
    "\n",
    "        c = depatchize(cc.cpu().detach().numpy().transpose(0, 2, 3, 1))\n",
    "        #warp = depatchize(ww.cpu().detach().numpy().transpose(0, 2, 3, 1))\n",
    "        c = np.clip(c, 0, 1) * 2 - 1.0\n",
    "        c = torch.from_numpy(c.transpose(2, 0, 1)).cuda().float()\n",
    "        c_batch.append(c)\n",
    "        \n",
    "    c_batch = torch.stack(c_batch)\n",
    "    return c_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import pickle\n",
    "# import cv2\n",
    "\n",
    "# class pixel_estimator_with_weights(nn.Module):\n",
    "#     def __init__(self, Weights,device = \"cpu\"):\n",
    "#         ## Default: gpu mode\n",
    "#         super(pixel_estimator_with_weights, self).__init__()\n",
    "#         self.device = torch.device(device)\n",
    "#         self.w1 = torch.from_numpy(Weights[0].transpose(3,2,0,1)).to(self.device)\n",
    "#         self.b1 = torch.from_numpy(Weights[1]).to(self.device)\n",
    "#         self.w2 = torch.tensor(Weights[2].transpose(3,2,0,1)).to(self.device)\n",
    "#         self.b2 = torch.tensor(Weights[3]).to(self.device)\n",
    "#         self.w3 = torch.tensor(Weights[4].transpose(3,2,0,1)).to(self.device)\n",
    "#         self.b3 = torch.tensor(Weights[5]).to(self.device)\n",
    "#         self.w4 = torch.tensor(Weights[6]).reshape(4,4,8,1024).permute(3,2,0,1).to(self.device)\n",
    "#         self.b4 = torch.tensor(Weights[7]).to(self.device)\n",
    "#         self.w5 = torch.tensor(Weights[8]).reshape(1,1,1024,512).permute(3,2,0,1).to(self.device)\n",
    "#         self.b5 = torch.tensor(Weights[9]).to(self.device)\n",
    "#         self.w6 = torch.tensor(Weights[10]).reshape(1,1,512,10).permute(3,2,0,1).to(self.device)\n",
    "#         self.b6 = torch.tensor(Weights[11]).to(self.device)\n",
    "#         self.w7 = torch.tensor(Weights[12]).reshape(1,1,10,1).permute(3,2,0,1).to(self.device)\n",
    "#         self.b7 = torch.tensor(Weights[13]).to(self.device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.conv2d(x,self.w1,bias = self.b1,stride=1))\n",
    "#         x = F.relu(F.conv2d(x,self.w2,bias = self.b2,stride=1,dilation=8))\n",
    "#         x = F.relu(F.conv2d(x,self.w3,bias = self.b3,stride=1,dilation=32))\n",
    "#         x = F.leaky_relu(F.conv2d(x,self.w4,bias = self.b4,stride=1,dilation=128),0.1)\n",
    "#         x = F.leaky_relu(F.conv2d(x,self.w5,bias = self.b5,stride=1),0.1)\n",
    "#         x = F.leaky_relu(F.conv2d(x,self.w6,bias = self.b6,stride=1),0.1)\n",
    "#         x = F.conv2d(x,self.w7,bias = self.b7,stride=1)\n",
    "#         return x\n",
    "    \n",
    "# AFmodel = torch.load('autofocus.pth',map_location='cpu')\n",
    "# AFmodel.eval()    \n",
    "    \n",
    "# def crop_patches(img, window= 1023, step = 512):\n",
    "#     patches = []\n",
    "#     H, W = img.shape\n",
    "#     for i in range(0, H-step, step):\n",
    "#         for j in range(0, W-step, step):\n",
    "#             patches.append(img[i:i+window, j:j+window])\n",
    "#     return np.stack(patches)\n",
    "\n",
    "\n",
    "# def gaf_func(img):\n",
    "#     assert img.max() <= 1.0\n",
    "#     assert img.shape == (2160, 3840)\n",
    "#     img = np.pad(img, ((200, 200), (128, 128)), 'reflect')\n",
    "#     H, W = img.shape\n",
    "    \n",
    "#     patches = crop_patches(img)\n",
    "#     patches = torch.from_numpy(patches).float().unsqueeze(1)#.cuda()\n",
    "        \n",
    "#     results = []\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(patches.size()[0]):\n",
    "#             results.append(AFmodel(patches[i:i+1]))\n",
    "#     results = torch.stack(results)\n",
    "\n",
    "#     results = results.numpy()#.cpu()\n",
    "#     results = results.squeeze()\n",
    "    \n",
    "#     k = 0\n",
    "#     sigma =1\n",
    "#     n_img = np.zeros((H-512, W-512))\n",
    "#     for i in range(0, H-512, 512):\n",
    "#         for j in range(0, W-512, 512):\n",
    "#             n_img[i:i+512, j:j+512] = results[k]\n",
    "#             k += 1\n",
    "    \n",
    "#     n_img[n_img < 0] = 0\n",
    "#     n_img = np.clip(n_img, 0, 8)\n",
    "#     return n_img\n",
    "\n",
    "# def gaf_func_tensor(Is):\n",
    "#     batch_size = Is.size(0)\n",
    "#     afs = []\n",
    "#     for i in range(batch_size):\n",
    "#         img = Is[i].cpu().numpy().transpose(1, 2, 0) * 127.5 + 127.5\n",
    "#         img = img.astype(np.uint8)\n",
    "#         img = cv2.resize(img, (3840, 2160))\n",
    "#         gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)/255.0\n",
    "#         gaf = gaf_func(gray_img)\n",
    "#         gaf = cv2.resize(gaf, (3072, 1536))\n",
    "#         gaf = (gaf-gaf.min())/(gaf.max()-gaf.min())*2.0-1.0\n",
    "#         gaf = torch.from_numpy(gaf).float().unsqueeze(0).cuda()\n",
    "# #         print(\"gaf: \", gaf.size(), gaf.max(), gaf.min())\n",
    "#         afs.append(gaf)\n",
    "        \n",
    "#     afs = torch.stack(afs)\n",
    "#     return afs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/home/qian/Downloads/DAVIS/test_davis_video_trunc_list.txt\"\n",
    "depth_path = \"/home/qian/Downloads/DAVIS/test_davis_dpt_trunc_list.txt\"\n",
    "video_dataset = load_davis_dataset(video_path, depth_path, 2)[0]\n",
    "# dataiter = iter(video_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    h = [torch.zeros(1, 1, 256).cuda(),\n",
    "                  torch.zeros(1, 1, 256).cuda()]\n",
    "    l = torch.rand(1, 2).cuda()*2.0-1.0 #-0.5~0.5\n",
    "    return h, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_rule\n",
    "apply_rule = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# x_train, dpt = dataiter.next()\n",
    "losses = []\n",
    "for i, (x_train, dpt) in enumerate(video_dataset):\n",
    "    print(i)\n",
    "    x_train = x_train.cuda()\n",
    "    dpt = dpt.cuda()\n",
    "# print(x_train.size())\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(x_train[0]).cpu())\n",
    "\n",
    "    # print(len(video_dataset))\n",
    "    # for i, (x_train, dpt) in enumerate(video_dataset):\n",
    "    #     print(i)\n",
    "\n",
    "    #     x_train = x_train.cuda()\n",
    "    #     dpt = dpt.cuda()\n",
    "    J_est = []\n",
    "    I_est = []\n",
    "    afs = []\n",
    "    locs = []\n",
    "    mus = []\n",
    "    J_prev = None\n",
    "    last_af = None\n",
    "    h, l = reset()\n",
    "    if apply_rule:\n",
    "        l = torch.ones(1, 1).cuda()*2.0#+1.0\n",
    "    else:\n",
    "        l = torch.zeros(1, 2).cuda()\n",
    "    with torch.no_grad():\n",
    "        for t in range(x_train.size(1)):\n",
    "#             print(\"seq:  \", t)\n",
    "            if apply_rule:\n",
    "                I, af, u_in  = utils_rule.getDefocuesImage(l, x_train[:, t, ...], dpt[:, t, ...])\n",
    "            else:\n",
    "                I, af, u_in  = getDefocuesImage(l, x_train[:, t, ...], dpt[:, t, ...])\n",
    "\n",
    "            if J_prev is None:\n",
    "                J_prev = I\n",
    "            else:\n",
    "                ## needs blockwise op\n",
    "                J_prev = image_fuse(I, J_prev)\n",
    "\n",
    "    #         print(af.size(), af.max(), af.min())\n",
    "#             af = gaf_func_tensor(J_prev)\n",
    "    #         print(af.size(), af.max(), af.min())\n",
    "\n",
    "            I_est.append(I)\n",
    "            J_est.append(J_prev)\n",
    "\n",
    "            last_af = af       \n",
    "            if last_af is None:\n",
    "                input_t = af\n",
    "            else:\n",
    "                input_t = torch.min(af, last_af)\n",
    "\n",
    "\n",
    "            afs.append(input_t)\n",
    "\n",
    "            if apply_rule:\n",
    "                l = utils_rule.rule_based(dpt[:, t, ...], l)\n",
    "            else:\n",
    "                h, mu, l, _, _ = model(input_t, l, h)\n",
    "                l = torch.rand(1, 2).cuda()*2.0-1.0\n",
    "            locs.append(l)\n",
    "            mus.append(mu)\n",
    "\n",
    "        J_est = torch.stack(J_est, dim = 1)\n",
    "        I_est = torch.stack(I_est, dim = 1)\n",
    "        afs = torch.stack(afs, dim = 1)\n",
    "        locs = torch.stack(locs, dim = 1)\n",
    "        mus = torch.stack(mus, dim = 1)\n",
    "        losses.append(F.mse_loss(J_est[:,1:], x_train[:,1:]).item())\n",
    "    \n",
    "\n",
    "# print(\"mse loss: \", F.mse_loss(J_est[:,1:], x_train[:,1:]).item())\n",
    "# print(\"ssim loss: \", pytorch_ssim.ssim(J_est[0,1:], x_train[0,1:]))\n",
    "# imshow(torchvision.utils.make_grid(color_region(I_est[0], locs[0])).cpu())\n",
    "# # # imshow(torchvision.utils.make_grid(U_est[0]).cpu())\n",
    "# imshow(torchvision.utils.make_grid(J_est[0]).cpu())\n",
    "# imshow(torchvision.utils.make_grid(afs[0]).cpu())\n",
    "# imshow(torchvision.utils.make_grid(x_train[0]).cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# s = int(time.time() % 10000)\n",
    "# for i in range(4):\n",
    "#     cv2.imwrite(\"{}_x_train_{}.jpg\".format(s, i), x_train[0, i].cpu().numpy().transpose(1, 2, 0)[..., ::-1] *127.5 + 127.5)\n",
    "#     cv2.imwrite(\"{}_J_est_{}.jpg\".format(s, i), J_est[0, i].cpu().numpy().transpose(1, 2, 0)[..., ::-1] *127.5 + 127.5)\n",
    "#     cv2.imwrite(\"{}_I_est{}.jpg\".format(s, i), I_est[0, i].cpu().numpy().transpose(1, 2, 0)[..., ::-1] *127.5 + 127.5)\n",
    "#     cv2.imwrite(\"{}_afs{}.jpg\".format(s, i), afs[0, i, 0].cpu().numpy()*127.5 + 127.5)\n",
    "#     cv2.imwrite(\"{}_afs_w{}.jpg\".format(s, i), color_region(afs[0].repeat(1, 3, 1, 1), mus[0])[i].cpu().numpy().transpose(1, 2, 0)[..., ::-1] *127.5 + 127.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
