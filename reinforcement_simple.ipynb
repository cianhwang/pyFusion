{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (20.0, 16.0)\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import mfnet, locnet, baselinenet\n",
    "from modules import retina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(78945)\n",
    "np.random.seed(78945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and\n",
    "    current value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImg(path):\n",
    "    \n",
    "    I = cv2.imread(path)\n",
    "    I = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "    I = I[:256, :256, :]\n",
    "    I = np.transpose(I, (-1, 0, 1))/127.5-1\n",
    "    \n",
    "    return I\n",
    "\n",
    "def dataloader(path, batch_size = 32):\n",
    "    y = []\n",
    "   \n",
    "    random_list = np.random.randint(1, 33, size=batch_size)\n",
    "\n",
    "    for i in random_list:\n",
    "        try:\n",
    "            yy = readImg(os.path.join(path, \"{:05d}_0.jpeg\".format(i)))\n",
    "        except:\n",
    "            print(\"errneous idx: \", i)\n",
    "        y.append(torch.Tensor(yy))\n",
    "\n",
    "    y = torch.stack(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roboticsnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(roboticsnet, self).__init__()\n",
    "        self._retina = retina()\n",
    "        self._mfnet = mfnet()\n",
    "        self._locnet = locnet()\n",
    "        self._baselinenet = baselinenet()\n",
    "        \n",
    "    def forward(self, x, J_prev, l_prev):\n",
    "        \n",
    "        X1 = self._retina.foveate(x, l_prev, isIt = True)\n",
    "        X2 = J_prev\n",
    "        \n",
    "        J = self._mfnet(X1, X2)\n",
    "        mu,l = self._locnet(J)\n",
    "        b = self._baselinenet(J).squeeze()\n",
    "        log_pi = torch.distributions.Normal(mu, 0.17).log_prob(l)\n",
    "        log_pi = torch.sum(log_pi, dim=1)\n",
    "        \n",
    "        return J, l, b, log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_number(net):\n",
    "    total_num = sum(p.numel() for p in net.parameters())\n",
    "    trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = roboticsnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"ckpt/model_ckpt.pth\")\n",
    "\n",
    "model._mfnet.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_freeze(model):\n",
    "    for name, child in model.named_children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "        dfs_freeze(child)\n",
    "dfs_freeze(model._mfnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roboticsnet(\n",
      "  (_mfnet): mfnet(\n",
      "    (_encoder): encoder(\n",
      "      (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (conv2): Conv2d(16, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
      "    )\n",
      "    (_decoder): decoder(\n",
      "      (deconv4): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (deconv5): ConvTranspose2d(64, 16, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (conv6): Conv2d(16, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (_locnet): locnet(\n",
      "    (_posnet): posnet(\n",
      "      (conv7): Conv2d(3, 16, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (conv8): Conv2d(16, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (conv9): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    )\n",
      "    (fc1): Linear(in_features=2048, out_features=16, bias=True)\n",
      "    (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      "  (_baselinenet): baselinenet(\n",
      "    (_posnet): posnet(\n",
      "      (conv7): Conv2d(3, 16, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (conv8): Conv2d(16, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (conv9): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    )\n",
      "    (fc3): Linear(in_features=2048, out_features=16, bias=True)\n",
      "    (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "{'Total': 873300, 'Trainable': 362483}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(batch_size = 32):\n",
    "    \n",
    "\n",
    "    y = dataloader('../datasets/highres_dataset2/', batch_size=batch_size)\n",
    "    foveate_fun = retina().foveate\n",
    "    l_J = torch.rand(batch_size, 2)*2-1\n",
    "    J_prev = foveate_fun(y, l_J, isIt = False)\n",
    "    \n",
    "    baselines = []\n",
    "    log_pi = []\n",
    "    l = torch.rand(batch_size, 2)*2-1\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    J, l, b, p = model(y, J_prev, l)\n",
    "    baselines.append(b)\n",
    "    log_pi.append(p)\n",
    "    \n",
    "    J, l, b, p = model(y, J, l)    \n",
    "    baselines.append(b)\n",
    "    log_pi.append(p)\n",
    "    \n",
    "    baselines = torch.stack(baselines).transpose(1, 0)\n",
    "    log_pi = torch.stack(log_pi).transpose(1, 0)\n",
    "    \n",
    "    R = -torch.mean((J - y)**2, [1, 2, 3])\n",
    "    R = 100 * R.unsqueeze(1).repeat(1, 2)\n",
    "\n",
    "    loss_baseline = F.mse_loss(baselines, R)\n",
    "    adjusted_reward = R - baselines.detach()\n",
    "\n",
    "    loss_reinforce = torch.sum(-log_pi*adjusted_reward, dim=1)\n",
    "    loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "    loss = loss_baseline+loss_reinforce\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.item(), torch.mean((J - y)**2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ,  0.8358496427536011 ,  0.0043095364\n",
      "1 ,  0.233819842338562 ,  0.004070033\n",
      "2 ,  0.2570072412490845 ,  0.0026859546\n",
      "3 ,  1.2966432571411133 ,  0.0055961064\n",
      "4 ,  0.1667121946811676 ,  0.004908211\n",
      "5 ,  0.28200656175613403 ,  0.0020898005\n",
      "6 ,  1.138702630996704 ,  0.005813714\n",
      "7 ,  0.23315289616584778 ,  0.0032000076\n",
      "8 ,  0.170571967959404 ,  0.0039642206\n",
      "9 ,  0.7207199335098267 ,  0.003944024\n"
     ]
    }
   ],
   "source": [
    "losses = AverageMeter()\n",
    "MSE = AverageMeter()\n",
    "for i in range(10):\n",
    "    loss, R = train_one_epoch()\n",
    "    losses.update(loss, 32)\n",
    "    MSE.update(-R, 32)\n",
    "    print(i, \", \", loss, \", \", R)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_fov = foveate_fun(y, l_J, isIt = True)\n",
    "# new_fov = foveate_fun(y, l, isIt = True)\n",
    "\n",
    "# idx = 0\n",
    "\n",
    "# fig, ax = plt.subplots(1, 3)\n",
    "# ax[0].imshow(np.transpose(J_p[idx].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# ax[1].imshow(np.transpose(J_p[idx+1].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# ax[2].imshow(np.transpose(J_p[idx+2].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# plt.show()\n",
    "\n",
    "# # fig, ax = plt.subplots(1, 3)\n",
    "# # ax[0].imshow(np.transpose(old_fov[idx].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# # ax[1].imshow(np.transpose(old_fov[idx+1].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# # ax[2].imshow(np.transpose(old_fov[idx+2].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# # plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(1, 3)\n",
    "# ax[0].imshow(np.transpose(new_fov[idx].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# ax[1].imshow(np.transpose(new_fov[idx+1].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# ax[2].imshow(np.transpose(new_fov[idx+2].cpu().numpy(), (1, 2, 0))/2+0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"===> training: \")\n",
    "# losses = AverageMeter()\n",
    "# mses = AverageMeter()\n",
    "# for epoch in range(10):\n",
    "#     tic = time.time()\n",
    "#     with tqdm(total=epoch) as pbar:\n",
    "\n",
    "#         for i in range(200):\n",
    "\n",
    "#             baselines = []\n",
    "#             log_pi = []\n",
    "# #             (X1, X2), y = dataloader('../datasets/highres_dataset2/')\n",
    "# #             foveate_fun = retina().foveate\n",
    "# #             l_J = torch.rand(32, 2)*2-1\n",
    "# #             J_prev = foveate_fun(y, l_J, isIt = False)\n",
    "#             l = torch.rand(32, 2)*2-1\n",
    "#             X1, X2, y, l = X1.to(device), X2.to(device), y.to(device), l.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             J, l, b, p = model(y, J_prev, l)\n",
    "#             baselines.append(b)\n",
    "#             log_pi.append(p)\n",
    "\n",
    "#             J, l, b, p = model(y, J, l)\n",
    "#             baselines.append(b)\n",
    "#             log_pi.append(p)\n",
    "\n",
    "#             baselines = torch.stack(baselines).transpose(1, 0)\n",
    "#             log_pi = torch.stack(log_pi).transpose(1, 0)\n",
    "\n",
    "#             ## one epoch training.\n",
    "#             R = ((J_prev - y).pow(2).mean(dim = (1, 2, 3))-(J - y).pow(2).mean(dim = (1, 2, 3)))\n",
    "#             R = R.unsqueeze(1).repeat(1, 2)\n",
    "#             loss_baseline = F.mse_loss(baselines, R)\n",
    "#             adjusted_reward = R - baselines.detach()\n",
    "#             loss_reinforce = torch.sum(-log_pi*adjusted_reward, dim=1)\n",
    "#             loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "#             loss = loss_baseline+loss_reinforce\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             toc = time.time()\n",
    "\n",
    "#             pbar.set_description(\n",
    "#                 (\n",
    "#                     \"{:.1f}s - loss: {:.3f} - mse: {:.4f}\".format(\n",
    "#                         (toc-tic), loss.item(), F.mse_loss(J, y).item()\n",
    "#                     )\n",
    "#                 )\n",
    "#             )\n",
    "#             pbar.update(32) \n",
    "            \n",
    "#             losses.update(loss.item(), 32)\n",
    "#             mses.update(F.mse_loss(J, y).item(), 32)\n",
    "        \n",
    "#     print(\"Epoch: {}/{} - training loss: {:.6f} - training mse: {:.6f}\".format(\n",
    "#                     epoch+1, 20, losses.avg, mses.avg))\n",
    "#     losses.reset()\n",
    "#     mses.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"ckpt_rl/rl_model_ckpt.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = {\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': loss\n",
    "#         }\n",
    "# torch.save(state, \"ckpt_rl/rl_model_ckpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     (X1, X2), y = dataloader('../datasets/highres_dataset2/', isTest = True)\n",
    "#     y_pred = model(X1.to(device), X2.to(device))\n",
    "#     y_features = model(X1.to(device), X2.to(device), rtrn_feature = True)\n",
    "    \n",
    "# y_prediction = y_pred.cpu().numpy()\n",
    "# y_prediction = np.transpose(y_prediction, (0, 2, 3, 1))\n",
    "# y_f = y_features.cpu().numpy()\n",
    "# y_f = np.transpose(y_f, (0, 2, 3, 1))\n",
    "# y = np.transpose(y, (0, 2, 3, 1))\n",
    "# X1 = np.transpose(X1, (0, 2, 3, 1))\n",
    "# X2 = np.transpose(X2, (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 7\n",
    "# fig, ax = plt.subplots(1, 5)\n",
    "# ax[0].imshow(y_prediction[idx]/2+0.5)\n",
    "# ax[1].imshow(y[idx]/2+0.5)\n",
    "# ax[2].imshow(y_f[idx, :, :, 0])\n",
    "# ax[3].imshow(X1[idx]/2+0.5)\n",
    "# ax[4].imshow(X2[idx]/2+0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
