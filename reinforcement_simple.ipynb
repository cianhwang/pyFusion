{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (20.0, 16.0)\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImg(path):\n",
    "    \n",
    "    I = cv2.imread(path)\n",
    "    I = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "    I = I[:256, :256, :]\n",
    "    I = np.transpose(I, (-1, 0, 1))/127.5-1\n",
    "    \n",
    "    return I\n",
    "\n",
    "def dataloader(path, batch_size = 32, isTest = False):\n",
    "    X1, X2, y = [], [], []\n",
    "#     random_list = np.random.randint(1, 8000, size=batch_size)\n",
    "    random_list = np.arange(1, batch_size+1)\n",
    "    if isTest:\n",
    "        random_list = np.random.randint(8001, 10000, size=batch_size)\n",
    "    for i in random_list:\n",
    "        try:\n",
    "            xx1 = readImg(os.path.join(path, \"{:05d}_1.jpeg\".format(i)))\n",
    "            xx2 = readImg(os.path.join(path, \"{:05d}_2.jpeg\".format(i)))\n",
    "            yy = readImg(os.path.join(path, \"{:05d}_0.jpeg\".format(i)))\n",
    "        except:\n",
    "            print(\"errneous idx: \", i)\n",
    "        y.append(torch.Tensor(yy))\n",
    "        X1.append(torch.Tensor(xx1))\n",
    "        X2.append(torch.Tensor(xx2))\n",
    "    X1 = torch.stack(X1)\n",
    "    X2 = torch.stack(X2)\n",
    "    y = torch.stack(y)\n",
    "    return (X1, X2), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class retina(object):\n",
    "    def foveate(self, x, lc):\n",
    "        ## camera model\n",
    "        B, C, H, W = x.shape\n",
    "        coors = (0.5 * ((lc + 1.0) * H)).long()\n",
    "        defocus_imgs = []\n",
    "        for i in range(B):\n",
    "            im = (x[i].cpu()+1)/2\n",
    "            im = np.transpose(im.numpy(), (1, 2, 0))\n",
    "            im = self.looknext(im, coors[i])\n",
    "            im = np.transpose(im, (-1, 0, 1))\n",
    "            im = torch.from_numpy(im).unsqueeze(dim=0)\n",
    "            defocus_imgs.append(im*2-1)\n",
    "        defocus_imgs = torch.cat(defocus_imgs).to(device)\n",
    "        \n",
    "        return defocus_imgs\n",
    "    \n",
    "    def looknext(self, I, coor, rad = 50, gaussKernelSize = 15):\n",
    "        # lc is a (mean, var) set of gaussian distribution\n",
    "#         print(\"I shape: \", I.shape)\n",
    "#         print(\"coor shape: \", coor.shape)\n",
    "        \n",
    "        x_c, y_c = coor[0], coor[1]\n",
    "\n",
    "        blurred_img = cv2.GaussianBlur(I, (gaussKernelSize, gaussKernelSize), 0)\n",
    "\n",
    "        mask = np.zeros(I.shape, dtype=np.uint8)\n",
    "        mask = cv2.circle(mask, (x_c, y_c), rad, (255,255,255), -1)\n",
    "\n",
    "        out = np.where(mask==np.array([255, 255, 255]), I, blurred_img)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 64, 5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, stride=4, padding=2)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super(decoder, self).__init__()\n",
    "            \n",
    "        self.deconv4 = nn.ConvTranspose2d(256, 64, 4, stride=4)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 16, 4, stride=4)\n",
    "        self.conv6 = nn.Conv2d(16, 1, 5, padding=2)        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.deconv4(x))\n",
    "        x = F.relu(self.deconv5(x))\n",
    "        x = torch.tanh(self.conv6(x))\n",
    "        x = (x+1)/2\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mfnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mfnet, self).__init__()\n",
    "        \n",
    "        self._encoder = encoder()\n",
    "        self._decoder = decoder()\n",
    "        \n",
    "    def forward(self, X1, X2, rtrn_feature = False):\n",
    "        x1 = self._encoder(X1)\n",
    "        x2 = self._encoder(X2)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = self._decoder(x)\n",
    "        if rtrn_feature:\n",
    "            return x\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        x = x * X1 + (1 - x) * X2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class locnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(locnet, self).__init__()\n",
    "\n",
    "        self.conv7 = nn.Conv2d(3, 16, 8, stride=8)\n",
    "        self.conv8 = nn.Conv2d(16, 32, 8, stride=8)\n",
    "        self.fc = nn.Linear(4*4*32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = x.view(-1, 4*4*32)\n",
    "        mu = torch.tanh(self.fc(x))\n",
    "        \n",
    "        noise = torch.zeros_like(mu)\n",
    "        noise.data.normal_(std=0.17)\n",
    "        l_t = mu + noise\n",
    "\n",
    "        # bound between [-1, 1]\n",
    "        l_t = torch.tanh(l_t)\n",
    "        return mu, l_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselinenet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(baselinenet, self).__init__()\n",
    "        self.conv9 = nn.Conv2d(3, 16, 8, stride=8)\n",
    "        self.conv10 = nn.Conv2d(16, 32, 8, stride=8)\n",
    "        self.fc2 = nn.Linear(4*4*32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = x.view(-1, 4*4*32)\n",
    "        b = self.fc2(x)\n",
    "        \n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roboticsnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(roboticsnet, self).__init__()\n",
    "        self._retina = retina()\n",
    "        self._mfnet = mfnet()\n",
    "        self._locnet = locnet()\n",
    "        self._baselinenet = baselinenet()\n",
    "        \n",
    "    def forward(self, x, J_prev, l_prev):\n",
    "        \n",
    "        X1 = self._retina.foveate(x, l_prev)\n",
    "        X2 = J_prev\n",
    "        \n",
    "        J = self._mfnet(X1, X2)\n",
    "        mu,l = self._locnet(J)\n",
    "        b = self._baselinenet(J).squeeze()\n",
    "        log_pi = torch.distributions.Normal(mu, 0.17).log_prob(l)\n",
    "        log_pi = torch.sum(log_pi, dim=1)\n",
    "        \n",
    "        return J, l, b, log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_number(net):\n",
    "    total_num = sum(p.numel() for p in net.parameters())\n",
    "    trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roboticsnet(\n",
      "  (_mfnet): mfnet(\n",
      "    (_encoder): encoder(\n",
      "      (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (conv2): Conv2d(16, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
      "    )\n",
      "    (_decoder): decoder(\n",
      "      (deconv4): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (deconv5): ConvTranspose2d(64, 16, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (conv6): Conv2d(16, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (_locnet): locnet(\n",
      "    (conv7): Conv2d(3, 16, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (conv8): Conv2d(16, 32, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (_baselinenet): baselinenet(\n",
      "    (conv9): Conv2d(3, 16, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (conv10): Conv2d(16, 32, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "{'Total': 584132, 'Trainable': 584132}\n"
     ]
    }
   ],
   "source": [
    "model = roboticsnet().to(device)\n",
    "print(model)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"ckpt/model_ckpt.pth\")\n",
    "\n",
    "model._mfnet.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0018,  0.1038, -0.0042, -0.0027, -0.0191, -0.0700, -0.3516,  0.1091,\n",
      "        -0.1582,  0.0110,  0.1276,  0.0568,  0.2201, -0.0012,  0.0005,  0.0027],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(list(model._mfnet.parameters())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_freeze(model):\n",
    "    for name, child in model.named_children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "        dfs_freeze(child)\n",
    "dfs_freeze(model._mfnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0018,  0.1038, -0.0042, -0.0027, -0.0191, -0.0700, -0.3516,  0.1091,\n",
      "        -0.1582,  0.0110,  0.1276,  0.0568,  0.2201, -0.0012,  0.0005,  0.0027],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(list(model._mfnet.parameters())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roboticsnet(\n",
      "  (_mfnet): mfnet(\n",
      "    (_encoder): encoder(\n",
      "      (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (conv2): Conv2d(16, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
      "    )\n",
      "    (_decoder): decoder(\n",
      "      (deconv4): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (deconv5): ConvTranspose2d(64, 16, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (conv6): Conv2d(16, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (_locnet): locnet(\n",
      "    (conv7): Conv2d(3, 16, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (conv8): Conv2d(16, 32, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (_baselinenet): baselinenet(\n",
      "    (conv9): Conv2d(3, 16, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (conv10): Conv2d(16, 32, kernel_size=(8, 8), stride=(8, 8))\n",
      "    (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "{'Total': 584132, 'Trainable': 73315}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# optimizer_loc = optim.Adam(model._locnet.parameters(), lr = 0.0005)\n",
    "# optimizer_baseline = optim.Adam(model._baselinenet.parameters(), lr = 0.0005)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baselines = []\n",
    "# log_pi = []\n",
    "# (X1, X2), y = dataloader('../datasets/highres_dataset2/', batch_size=8)\n",
    "# l = torch.rand(8, 2)*2-1\n",
    "\n",
    "# X1, X2, y, l = X1.to(device), X2.to(device), y.to(device), l.to(device)\n",
    "\n",
    "# # optimizer_loc.zero_grad()\n",
    "# # optimizer_baseline.zero_grad()\n",
    "# optimizer.zero_grad()\n",
    "# J, l, b, p = model(y, X1, l)\n",
    "# # print(\"J.shape: \", J.shape)\n",
    "# # print(\"l.shape: \", l.shape)\n",
    "# # print(\"b.shape: \", b.shape)\n",
    "# # print(\"p.shape: \", p.shape)\n",
    "# baselines.append(b)\n",
    "# log_pi.append(p)\n",
    "# baselines = torch.stack(baselines).transpose(1, 0)\n",
    "# log_pi = torch.stack(log_pi).transpose(1, 0)\n",
    "# # print(\"baselines.shape: \", baselines.shape)\n",
    "# # print(\"log_pi.shape: \", log_pi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## one epoch training.\n",
    "# R = -(J - y).pow(2).mean(dim = (1, 2, 3))\n",
    "# R = R.unsqueeze(1)\n",
    "# loss_baseline = F.mse_loss(baselines, R)\n",
    "# adjusted_reward = R - baselines.detach()\n",
    "# loss_reinforce = torch.sum(-log_pi*adjusted_reward, dim=1)\n",
    "# loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "# loss = loss_baseline+loss_reinforce\n",
    "# loss.backward()\n",
    "# print(loss_baseline.item())\n",
    "# print(loss_reinforce.item())\n",
    "# print(F.mse_loss(J, y).item())\n",
    "# # optimizer_loc.step()\n",
    "# # optimizer_baseline.step()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> training: \n",
      "loss: 0.000036, mse: 0.011044\n",
      "loss: 0.005253, mse: 0.010868\n",
      "loss: 0.003953, mse: 0.010520\n",
      "loss: 0.003933, mse: 0.010640\n",
      "loss: 0.006145, mse: 0.011329\n",
      "loss: 0.004122, mse: 0.010633\n",
      "loss: 0.002841, mse: 0.010948\n",
      "loss: 0.003580, mse: 0.010475\n",
      "loss: -0.000318, mse: 0.010708\n",
      "loss: -0.002629, mse: 0.010557\n",
      "loss: -0.000605, mse: 0.011103\n",
      "loss: -0.003581, mse: 0.010965\n",
      "loss: -0.004003, mse: 0.010421\n",
      "loss: -0.004489, mse: 0.010641\n",
      "loss: -0.004803, mse: 0.010279\n",
      "loss: -0.002946, mse: 0.010331\n",
      "loss: -0.005527, mse: 0.010168\n",
      "loss: -0.002073, mse: 0.010683\n",
      "loss: -0.004416, mse: 0.011315\n",
      "loss: -0.002276, mse: 0.010136\n",
      "loss: 0.001261, mse: 0.011061\n",
      "loss: 0.001781, mse: 0.010783\n",
      "loss: -0.000052, mse: 0.010105\n",
      "loss: -0.001537, mse: 0.010623\n",
      "loss: 0.000140, mse: 0.010342\n",
      "loss: 0.000727, mse: 0.011172\n",
      "loss: 0.000210, mse: 0.010121\n",
      "loss: 0.001032, mse: 0.010457\n",
      "loss: 0.000369, mse: 0.010498\n",
      "loss: 0.000795, mse: 0.010482\n",
      "loss: 0.000812, mse: 0.010761\n",
      "loss: 0.000645, mse: 0.010932\n",
      "loss: -0.000033, mse: 0.010270\n",
      "loss: 0.000562, mse: 0.010938\n",
      "loss: -0.001049, mse: 0.010692\n",
      "loss: -0.002439, mse: 0.010004\n",
      "loss: 0.000232, mse: 0.010876\n",
      "loss: -0.000777, mse: 0.010621\n",
      "loss: -0.002008, mse: 0.010210\n",
      "loss: -0.000446, mse: 0.011150\n",
      "loss: 0.001177, mse: 0.010474\n",
      "loss: 0.000191, mse: 0.010351\n",
      "loss: 0.001372, mse: 0.011019\n",
      "loss: 0.001529, mse: 0.010423\n",
      "loss: 0.000857, mse: 0.010644\n",
      "loss: -0.000180, mse: 0.010939\n",
      "loss: 0.000879, mse: 0.010949\n",
      "loss: -0.002151, mse: 0.010379\n",
      "loss: 0.001146, mse: 0.010707\n",
      "loss: -0.000901, mse: 0.010647\n",
      "loss: -0.000855, mse: 0.010513\n",
      "loss: -0.001145, mse: 0.010762\n",
      "loss: -0.001034, mse: 0.010604\n",
      "loss: -0.001126, mse: 0.010444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a91b2240c8da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlog_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlog_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-522f86b80433>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, J_prev, l_prev)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfoveate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ec0c4f9b768d>\u001b[0m in \u001b[0;36mfoveate\u001b[0;34m(self, x, lc)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlooknext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ec0c4f9b768d>\u001b[0m in \u001b[0;36mlooknext\u001b[0;34m(self, I, coor, rad, gaussKernelSize)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mblurred_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgaussKernelSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussKernelSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"===> training: \")\n",
    "for epoch in range(10):\n",
    "\n",
    "    for i in range(20):\n",
    "\n",
    "        baselines = []\n",
    "        log_pi = []\n",
    "        (X1, X2), y = dataloader('../datasets/highres_dataset2/')\n",
    "        l = torch.rand(32, 2)*2-1\n",
    "        X1, X2, y, l = X1.to(device), X2.to(device), y.to(device), l.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        J, l, b, p = model(y, X2, l)\n",
    "        baselines.append(b)\n",
    "        log_pi.append(p)\n",
    "        \n",
    "        J, l, b, p = model(y, J, l)\n",
    "        baselines.append(b)\n",
    "        log_pi.append(p)\n",
    "        \n",
    "        baselines = torch.stack(baselines).transpose(1, 0)\n",
    "        log_pi = torch.stack(log_pi).transpose(1, 0)\n",
    "        \n",
    "        ## one epoch training.\n",
    "        R = -(J - y).pow(2).mean(dim = (1, 2, 3))\n",
    "        R = R.unsqueeze(1).repeat(1, 2)\n",
    "        loss_baseline = F.mse_loss(baselines, R)\n",
    "        adjusted_reward = R - baselines.detach()\n",
    "        loss_reinforce = torch.sum(-log_pi*adjusted_reward, dim=1)\n",
    "        loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "        loss = loss_baseline+loss_reinforce\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        print(\"loss: {:4f}, mse: {:4f}\".format(loss.item(), F.mse_loss(J, y).item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     (X1, X2), y = dataloader('../datasets/highres_dataset2/', isTest = True)\n",
    "#     y_pred = model(X1.to(device), X2.to(device))\n",
    "#     y_features = model(X1.to(device), X2.to(device), rtrn_feature = True)\n",
    "    \n",
    "# y_prediction = y_pred.cpu().numpy()\n",
    "# y_prediction = np.transpose(y_prediction, (0, 2, 3, 1))\n",
    "# y_f = y_features.cpu().numpy()\n",
    "# y_f = np.transpose(y_f, (0, 2, 3, 1))\n",
    "# y = np.transpose(y, (0, 2, 3, 1))\n",
    "# X1 = np.transpose(X1, (0, 2, 3, 1))\n",
    "# X2 = np.transpose(X2, (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 7\n",
    "# fig, ax = plt.subplots(1, 5)\n",
    "# ax[0].imshow(y_prediction[idx]/2+0.5)\n",
    "# ax[1].imshow(y[idx]/2+0.5)\n",
    "# ax[2].imshow(y_f[idx, :, :, 0])\n",
    "# ax[3].imshow(X1[idx]/2+0.5)\n",
    "# ax[4].imshow(X2[idx]/2+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
